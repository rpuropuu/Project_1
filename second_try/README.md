![](https://img.shields.io/badge/Python-3.9-blue)
![](https://img.shields.io/badge/tensorflow-2.4.1-blue)
![](https://img.shields.io/badge/NumPy-1.19.5-blue)
![](https://img.shields.io/badge/matplotlib-3.2.2-blue)
![](https://img.shields.io/badge/cv2-4.1.2-blue)
![](https://img.shields.io/badge/scikit-0.22.2.post1-blue)


__________


После дополнения условия первичного задания (изменён ввод данных), я перезапустил обучение обеих нейронок. Вариант с использованием VGG делать не стал, как оказалось, у него итоговый коэфициент классификации обеъекта был ниже: **91%** против **97%** на своей предобученной. Для обучения использовались те же файлы, что и в первичном задании: [01_catog.ipynb](https://github.com/rpuropuu/Project_1/blob/main/first_try/01_catog.ipynb) и [02_catog.ipynb](https://github.com/rpuropuu/Project_1/blob/main/first_try/02_catog.ipynb)


_________


**Этот абзац более не имеет силы** оставлен, так как получен положительный исследовательский опыт, для сравнения ожиданий, и почему не всё оказалось так, как ожидалось:



Новый датасет содержал тех же **3385** картинок, только теперь был заранее разбит на обучающую и валидационную выборки: **2985** + **400**. Скрипт на рандомное разделение теперь оказался не нужен - меньше работы). Предыдущий самостоятельно дополненный вариант выборки, на **8000** картинок с выравниванием соотношений классов 1 к 1, изначально обучился на **86%** и при дообучении показал **лучший результат - 91%**. Вариант сети с новой недополненной выборкой повёл себя несколько неожиданно для меня: первая сеть обучилась на тех же **86%**, а вот во второй показатель вырос с 91% **до 97%**! Да и к тому же обучалась заметно быстрее. Я даже сократил количество эпох до **5** (ранее было 15) Теперь настало время визуализировать разницу, ведь коробки обучилис на **70%** вместо **80%** ранее заявленных генератором. По факту средний коэфициент mIoU был итого ниже - 57%.


_________


Итак, корректным осталась цифра обучения классификации - **86%**, а вот то что вторая сеть обучилась на 97% оказалось заблуждением. После того, как я не увидел изменений в обучений первого рандомного поколения трейнниг датасета, с заранее подготовленным и уже разбитым для тренировки, я решил второй слой обучить по рандому на всю новую выборку, снова свой рандом - это была ошибка. Оказалось, что после рандома некоторые фотографии из тестовой выборки попали в тренировочную, и это взвентило счётчик всего за пять эпох. Для настолько маленькой выборки это оказалось через чур заметно. Далее, при визуализации, прояснились и другие неприятные изыскания, давайте посмотрим на примеры, заранее скажу, что mIoU снизилось с 57% до 36%, позже, когда заменил выборку на изначальную в 400 кратинок, получилось 37%. Итак, смотрим:


![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/20.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/25.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/24.jpg)


Синие боксы предсказания уж сильно большие, сразу и не поймёшь что именно не так. Смотрим ещё:


![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/27.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/26.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/28.jpg)


Тут было получше, дальше немного ошибок классификации:


![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/22.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/21.jpg)


Явно боксы не попадают потому что просто берутся крупные участки по центру, а вытягивание вдоль силуета объекта класса просто совпадает с геометрией исходной картинки, однако, щаметно и положительное смещение бокса от центра, в сторону лиц животных. Решил увеличить количество эпох назад до 15, все 15 эпох val_bounding_box_accuracy: 0.7253. При это вал_бокс_лосс имел совсем слабую тенденцию к понижению с 0.0242 до  0.0208. Результат не оказал положительного эффекта на боксы. Явно настройки, подошедшие для тренировки картинок, сюда не подходят. Смена настройки оптимайзера, на изначальные давшие mIoU в 57%, тоже ничего не дала, оставив все тех же 37%. Требуются дальнейшая исследовательская работа. Почему же, при использовании разных функций потерь для классификации и боксов, использование своей предобученой сети вместо VGG начинает так слабо влиять на обучаемость конкретно локализации.


И всё же, вот итоговый результат:


> mIoU 37%, classification accuracy 89%, 0.054 ms, 2985 train, 400 valid.
 

При инициализации с одной кратинкой время выростает до 0,1 ms. 


Исследовательская часть ведётся на TPU в Google Colaboratory, а обработка результатов производится в локальной среде на домашнем компьютере. По окончании экспериментов, я создам отдельную папку со скриптами, позволяющими подготовить и визуализировать результаты. (0_config.py, 1_predict_mIoU.py, 2_compile_data_dor_mIoU.py, 3_vizualize_results.py) А пока я их часто переделываю, ещё в процессе оптимизации. Сюда и в предыдущий [README.md](https://github.com/rpuropuu/Project_1/blob/main/first_try/README.md) будут добавлены ссылки на папку с ними. 


_Есть мысли заделать **DenseNet**._
