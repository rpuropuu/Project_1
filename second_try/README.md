![](https://img.shields.io/badge/Python-3.9-blue)
![](https://img.shields.io/badge/tensorflow-2.4.1-blue)
![](https://img.shields.io/badge/NumPy-1.19.5-blue)
![](https://img.shields.io/badge/matplotlib-3.2.2-blue)
![](https://img.shields.io/badge/cv2-4.1.2-blue)
![](https://img.shields.io/badge/scikit-0.22.2.post1-blue)


__________


После дополнения условия первичного задания (изменён ввод данных), я перезапустил обучение обеих нейронок. Вариант с использованием VGG делать не стал, как оказалось, у него итоговый коэфициент классификации обеъекта был ниже: **91%** против **97%** на своей предобученной. Для обучения использовались те же файлы, что и в первичном задании: [01_catog.ipynb](https://github.com/rpuropuu/Project_1/blob/main/first_try/01_catog.ipynb) и [02_catog.ipynb](https://github.com/rpuropuu/Project_1/blob/main/first_try/02_catog.ipynb)


_________


**Этот абзац более не имеет силы** оставлен, так как получен положительный исследовательский опыт, для сравнения ожиданий, и почему не всё оказалось так, как ожидалось:



Новый датасет содержал тех же **3385** картинок, только теперь был заранее разбит на обучающую и валидационную выборки: **2985** + **400**. Скрипт на рандомное разделение теперь оказался не нужен - меньше работы). Предыдущий самостоятельно дополненный вариант выборки, на **8000** картинок с выравниванием соотношений классов 1 к 1, изначально обучился на **86%** и при дообучении показал **лучший результат - 91%**. Вариант сети с новой недополненной выборкой повёл себя несколько неожиданно для меня: первая сеть обучилась на тех же **86%**, а вот во второй показатель вырос с 91% **до 97%**! Да и к тому же обучалась заметно быстрее. Я даже сократил количество эпох до **5** (ранее было 15) Теперь настало время визуализировать разницу, ведь коробки обучилис на **70%** вместо **80%** ранее заявленных генератором. По факту средний коэфициент mIoU был итого ниже - 57%.


_________


Итак, корректным осталась цифра обучения классификации - **86%**, а вот то что вторая сеть обучилась на 97% оказалось заблуждением. После того, как я не увидел изменений в обучений первого рандомного поколения трейнниг датасета, с заранее подготовленным и уже разбитым для тренировки, я решил второй слой обучить по рандому на всю новую выборку, снова свой рандом - это была ошибка. Оказалось, что после рандома некоторые фотографии из тестовой выборки попали в тренировочную, и это взвентило счётчик всего за пять эпох. Для настолько маленькой выборки это оказалось через чур заметно. Далее, при визуализации, прояснились и другие неприятные изыскания, давайте посмотрим на примеры, заранее скажу, что mIoU снизилось с 57% до 36%, позже, когда заменил выборку на изначальную в 400 кратинок, получилось 37%. Итак, смотрим:


![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/20.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/25.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/24.jpg)


Синие боксы предсказания уж сильно большие, сразу и не поймёшь что именно не так. Смотрим ещё:


![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/27.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/28.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/26.jpg)


Тут было получше, дальше немного ошибок классификации:


![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/22.jpg)
![](https://github.com/rpuropuu/Project_1/blob/main/second_try/data/21.jpg)

